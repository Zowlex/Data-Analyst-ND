{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling report\n",
    "<hr>\n",
    "\n",
    "**May 28, 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "   - Wrangling of the WeRateDogs dataset from the tweet archive of twitter user @dog_rates was made of 4+1 steps which will be discussed individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data gathering for this project was challenging, we were asked to gather data from 3 different sources using 3 different methods. \n",
    "\n",
    "First, I manually downloaded the WeRateDogs twitter archive file provided by instructors: `twitter-archive-enhanced.csv`.\n",
    "\n",
    "Then, I downloaded programmatically the tweet image predictions file `image-predictions.tsv` from this [url](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) using the **requests** library and stored it locally as a tsv file. All of this was wrapped in a single function `get_file(url)`.\n",
    "\n",
    "Finally, I Scraped likes, retweet counts and additional data for each tweet_id present in the tweet archive above using Tweepy and this was totally new for me and was made in various steps (All of the steps were perfectly provided in section4 of the project page):\n",
    "   1. I created a twitter developer account, and submitted an application to access the twitter api as a student. \n",
    "   2. I requested my API key and secret token and stored them as environment variables for security reasons.\n",
    "   3. For each tweet_id present in `twitter-archive-enhanced.csv` file I scraped its entire JSON data and saved it as a txt file `tweet_json.txt`\n",
    "   4. From the file `tweet_json.txt` I extracted each tweet's favorite_count and retweet_count and stored it in a pandas dataframe `tweet_json_df` and then saved it as a csv file `tweet_extra.csv`\n",
    "   \n",
    "As a result, we ended up with 3 different files to work on:\n",
    "   - twitter-archive-enhanced.csv\n",
    "   - image-predictions.tsv\n",
    "   - tweet_extra.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data Assessment, we went through each file and assessed it visually and programmatically using pandas and its different functions (.describe(), .isnull(), .apply(), etc...) .I spotted **9 quality** problems and **3 tidiness** problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data cleaning, I read the 3 csv files and made copies with prefix *_clean* to work on and/or go back to this step if something didn't go right. Then I **defined** each table's problems, then **cleaned** each problem individually and **tested** it in the test section. After, cleaning each table individually I merged them by `tweet_id` as a single dataframe `tweet_arch_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Predicting missing dog stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While cleaning data I noticed that a large portion of the dataset's names and dog stages were missing. I couldn't do anything about missing names but for the missing dog stages (1724 missing entries) I tried to **predict the missing values**. I split the data into **train** (where the dog stages are not missing) and **test** (where dog_stage is missing) and set up a classifier (this a **multiclass classification problem** because we are trying to predict each dog's dog stage which could be pupper, puppo, floof or doggo).\n",
    "\n",
    "I used a **catboost** classifier with **raw features** and was able to predict the dog stage with **~70% accuracy**.\n",
    "\n",
    "N.B: This step was a personal effort and a mini challenge otherwise I could've left missing dog stages as None or dropped them from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of data wrangling is to store the clean file(s), it was made using two different methods. First, I wrote the clean dataframe `tweet_arch_clean` to a csv file `twitter_archive_master.csv`.\n",
    "Then, I stored the clean dataframe `tweet_arch_clean` as a SQLite database `twitter_archive.db` file using **sqlalchemy** library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
